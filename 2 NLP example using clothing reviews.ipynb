{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETUP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SETUP:\n",
    "    !pip install -q -U toai\n",
    "    !pip install -q -U nb_black\n",
    "    !pip install -q -U tensorflow-datasets\n",
    "    !pip install -q -U --no-deps tensorflow-addons\n",
    "    !pip install -q -U tensorflow_hub\n",
    "    print(__import__(\"toai\").__version__)\n",
    "    print(__import__(\"tensorflow\").__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__import__(\"toai\").__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toai.imports import *\n",
    "from toai.utils import save_file, load_file\n",
    "from toai.data import DataContainer, DataBundle\n",
    "from toai.metrics import sparse_top_2_categorical_accuracy\n",
    "from toai.models import save_keras_model, load_keras_model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data/womens-ecommerce-clothing-reviews\")\n",
    "TEMP_DIR = Path(\"temp/womens-ecommerce-clothing-reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SETUP:\n",
    "    shutil.rmtree(str(DATA_DIR), ignore_errors=True)\n",
    "    shutil.rmtree(str(TEMP_DIR), ignore_errors=True)\n",
    "    DATA_DIR.mkdir(parents=True)\n",
    "    TEMP_DIR.mkdir(parents=True)\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_files(\n",
    "        dataset=\"nicapotato/womens-ecommerce-clothing-reviews\",\n",
    "        path=DATA_DIR,\n",
    "        unzip=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(\n",
    "    DATA_DIR / \"Womens Clothing E-Commerce Reviews.csv\", low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_values(df, col_name, values):\n",
    "    return df.loc[~df[col_name].isin(values), :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rare_values(df, col_name, threshold):\n",
    "    counts = df[col_name].value_counts(normalize=True)\n",
    "    return df.loc[df[col_name].isin(counts[counts > threshold].index), :].reset_index(\n",
    "        drop=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_data = all_data[~all_data[\"Review Text\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_data_bundle = DataBundle.from_dataframe(\n",
    "    dataframe=available_data, x_col=\"Review Text\", y_col=\"Rating\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = DataBundle.split(\n",
    "    data_bundle=available_data_bundle, fracs=(0.8, 0.1, 0.1), random=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = train_data.make_label_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(label_map, TEMP_DIR / \"label_map.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = load_file(TEMP_DIR / \"label_map.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.apply_label_map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.apply_label_map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.apply_label_map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {0: 653, 1: 1241, 2: 2278, 3: 3861, 4: 10080}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = dict(\n",
    "    enumerate(\n",
    "        sk.utils.class_weight.compute_class_weight(\n",
    "            \"balanced\", np.unique(train_data.y), train_data.y\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataBundle.from_unbalanced(train_data, 5000, train_data.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s(auto_attribs=True)\n",
    "class TextPreprocessor:\n",
    "    max_length: int = 100\n",
    "    default_value: str = b\"<pad>\"\n",
    "\n",
    "    def __call__(self, text: tf.Tensor, label: tf.Tensor) -> tf.Tensor:\n",
    "        text = tf.strings.regex_replace(text, b\"[^a-zA-Z']\", b\" \")\n",
    "        text = tf.strings.split(text)\n",
    "        text = text[:, : self.max_length]\n",
    "        return text.to_tensor(default_value=self.default_value), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset = (\n",
    "    train_data.to_dataset()\n",
    "    .shuffle(len(train_data))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(TextPreprocessor(), num_parallel_calls=AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocabulary(dataset):\n",
    "    vocabulary = Counter()\n",
    "    for x, _ in dataset:\n",
    "        for review in x:\n",
    "            vocabulary.update(review.numpy().tolist())\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = make_vocabulary(base_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:VOCABULARY_SIZE]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(truncated_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in b\"it was the most amazing dress\".split():\n",
    "    print(word_to_id.get(word) if word_to_id.get(word) is not None else VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_oov_buckets = VOCABULARY_SIZE // 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_oov_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(words, word_ids), n_oov_buckets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.lookup(tf.constant([b\"These shoes are very patogus\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s(auto_attribs=True)\n",
    "class WordEncoder:\n",
    "    vocabulary_table: tf.lookup.StaticVocabularyTable\n",
    "\n",
    "    def __call__(self, text: tf.Tensor, labels: tf.Tensor) -> tf.Tensor:\n",
    "        return self.vocabulary_table.lookup(text), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    train_data.to_dataset()\n",
    "    .shuffle(len(train_data))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(TextPreprocessor(), num_parallel_calls=AUTOTUNE)\n",
    "    .map(WordEncoder(vocabulary_table=table), num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = (\n",
    "    valid_data.to_dataset()\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(TextPreprocessor(), num_parallel_calls=AUTOTUNE)\n",
    "    .map(WordEncoder(vocabulary_table=table), num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = (\n",
    "    test_data.to_dataset()\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(TextPreprocessor(), num_parallel_calls=AUTOTUNE)\n",
    "    .map(WordEncoder(vocabulary_table=table), num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_container = DataContainer(\n",
    "    base=train_dataset,\n",
    "    train=train_dataset,\n",
    "    train_steps=math.ceil(len(train_data) / BATCH_SIZE),\n",
    "    validation=valid_dataset,\n",
    "    test=test_dataset,\n",
    "    label_map=label_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in data_container.train.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(x[0])\n",
    "    print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in data_container.validation.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(x[0])\n",
    "    print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequential_lstm_model(\n",
    "    n_categories, embedding_size, lstm_size, lstm_dropout, dropout\n",
    "):\n",
    "    return keras.models.Sequential(\n",
    "        [\n",
    "            keras.layers.Embedding(\n",
    "                VOCABULARY_SIZE + n_oov_buckets,\n",
    "                embedding_size,\n",
    "                mask_zero=True,\n",
    "                input_shape=[None],\n",
    "            ),\n",
    "            keras.layers.Bidirectional(\n",
    "                keras.layers.LSTM(\n",
    "                    lstm_size, dropout=lstm_dropout, return_sequences=True\n",
    "                )\n",
    "            ),\n",
    "            keras.layers.Bidirectional(\n",
    "                keras.layers.LSTM(\n",
    "                    lstm_size, dropout=lstm_dropout, return_sequences=False\n",
    "                )\n",
    "            ),\n",
    "            #             keras.layers.GlobalMaxPool1D(),\n",
    "            keras.layers.Dropout(dropout),\n",
    "            keras.layers.Dense(n_categories, activation=keras.activations.softmax),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_sequential_lstm_model(\n",
    "    n_categories=data_container.n_classes,\n",
    "    embedding_size=256,\n",
    "    lstm_size=256,\n",
    "    lstm_dropout=0.1,\n",
    "    dropout=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.sparse_categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.Adam(lr=3e-4),\n",
    "    metrics=[\n",
    "        keras.metrics.sparse_categorical_accuracy,\n",
    "        sparse_top_2_categorical_accuracy,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    data_container.train,\n",
    "    steps_per_epoch=data_container.train_steps // 5,\n",
    "    validation_data=data_container.validation,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=1, factor=0.3),\n",
    "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(data_container.validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        np.concatenate([y.numpy() for _, y in data_container.validation]),\n",
    "        model.predict(data_container.validation).argmax(axis=1),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    train_data.to_dataset()\n",
    "    .shuffle(len(train_data))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = valid_data.to_dataset().batch(BATCH_SIZE).cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_data.to_dataset().batch(BATCH_SIZE).cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_container = DataContainer(\n",
    "    base=train_dataset,\n",
    "    train=train_dataset,\n",
    "    train_steps=math.ceil(len(train_data) / BATCH_SIZE),\n",
    "    validation=valid_dataset,\n",
    "    test=test_dataset,\n",
    "    label_map=label_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    data_container,\n",
    "    epochs,\n",
    "    lrs=None,\n",
    "    optimizers=None,\n",
    "    patience=5,\n",
    "    class_weights=None,\n",
    "    verbose=1,\n",
    "    log_dir=str(TEMP_DIR / \"logs\"),\n",
    "):\n",
    "    if optimizers is None:\n",
    "        optimizers = [keras.optimizers.Adam(lr) for lr in lrs]\n",
    "    model.compile(\n",
    "        loss=keras.losses.sparse_categorical_crossentropy,\n",
    "        optimizer=optimizers[0],\n",
    "        metrics=[\n",
    "            keras.metrics.sparse_categorical_accuracy,\n",
    "            sparse_top_2_categorical_accuracy,\n",
    "        ],\n",
    "    )\n",
    "    model.fit(\n",
    "        data_container.train,\n",
    "        steps_per_epoch=data_container.train_steps,\n",
    "        validation_data=data_container.validation,\n",
    "        epochs=epochs[0],\n",
    "        callbacks=[\n",
    "            keras.callbacks.ReduceLROnPlateau(patience=patience, factor=0.3),\n",
    "            keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True),\n",
    "        ],\n",
    "        class_weight=class_weights,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    model.layers[0].trainable = True\n",
    "    model.compile(\n",
    "        loss=keras.losses.sparse_categorical_crossentropy,\n",
    "        optimizer=optimizers[1],\n",
    "        metrics=[\n",
    "            keras.metrics.sparse_categorical_accuracy,\n",
    "            sparse_top_2_categorical_accuracy,\n",
    "        ],\n",
    "    )\n",
    "    model.fit(\n",
    "        data_container.train,\n",
    "        steps_per_epoch=data_container.train_steps,\n",
    "        validation_data=data_container.validation,\n",
    "        epochs=epochs[1],\n",
    "        callbacks=[\n",
    "            keras.callbacks.ReduceLROnPlateau(patience=patience // 2, factor=0.3),\n",
    "            keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True),\n",
    "            keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "        ],\n",
    "        class_weight=class_weights,\n",
    "        verbose=verbose,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hub_model(url, n_categories):\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            hub.KerasLayer(url, dtype=tf.string, input_shape=[]),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            keras.layers.Dense(n_categories, activation=keras.activations.softmax),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(urls, data_container, class_weights):\n",
    "    for url in urls:\n",
    "        model = make_hub_model(url, data_container.n_classes)\n",
    "        model_name = f\"{url.split('/')[4]}\"\n",
    "        print(f\" {model_name} \".center(80, \"=\"))\n",
    "        shutil.rmtree(str(TEMP_DIR / model_name), ignore_errors=True)\n",
    "        train_model(\n",
    "            model=model,\n",
    "            data_container=data_container,\n",
    "            epochs=[25, 15],\n",
    "            optimizers=[keras.optimizers.Adam(lr=3e-4), keras.optimizers.Adam(lr=1e-4)],\n",
    "            class_weights=class_weights,\n",
    "            patience=4,\n",
    "            verbose=2,\n",
    "            log_dir=str(TEMP_DIR / model_name),\n",
    "        )\n",
    "        model.save(f\"{TEMP_DIR / model_name}.h5\")\n",
    "        save_keras_model(\n",
    "            model,\n",
    "            str(TEMP_DIR / model_name / \"architecture\"),\n",
    "            str(TEMP_DIR / model_name / \"weights\"),\n",
    "        )\n",
    "        keras.backend.clear_session()\n",
    "        del model\n",
    "        keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(urls, data_container):\n",
    "    reports = {}\n",
    "    for url in urls:\n",
    "        model_name = f\"{url.split('/')[4]}\"\n",
    "        print(f\" {model_name} \".center(80, \"=\"))\n",
    "        try:\n",
    "            model = keras.model.load_model(\n",
    "                f\"{TEMP_DIR / model_name}.h5\",\n",
    "                custom_objects={\"KerasLayer\": hub.KerasLayer},\n",
    "            )\n",
    "        except:\n",
    "            print(f\"Loading architecture & weights separately\")\n",
    "            model = load_keras_model(\n",
    "                str(TEMP_DIR / model_name / \"architecture\"),\n",
    "                str(TEMP_DIR / model_name / \"weights\"),\n",
    "                custom_objects={\"KerasLayer\": hub.KerasLayer},\n",
    "            )\n",
    "        reports[model_name] = classification_report(\n",
    "            [\n",
    "                label.numpy()\n",
    "                for _, label in data_container.validation.take(-1).unbatch()\n",
    "            ],\n",
    "            model.predict(data_container.validation).argmax(axis=1),\n",
    "        )\n",
    "        del model\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_urls = (\n",
    "    \"https://tfhub.dev/google/Wiki-words-250/2\",\n",
    "    \"https://tfhub.dev/google/Wiki-words-250-with-normalization/2\",\n",
    "    \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models(model_urls, data_container, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = evaluate_models(model_urls, data_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, report in reports.items():\n",
    "    print(f\" {model_name} \".center(80, \"=\"))\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================ Wiki-words-250 ================================\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.54      0.81      0.65        77\n",
    "#            1       0.52      0.68      0.59       159\n",
    "#            2       0.54      0.60      0.57       303\n",
    "#            3       0.48      0.54      0.51       500\n",
    "#            4       0.86      0.73      0.79      1226\n",
    "\n",
    "#     accuracy                           0.67      2265\n",
    "#    macro avg       0.59      0.67      0.62      2265\n",
    "# weighted avg       0.70      0.67      0.68      2265\n",
    "\n",
    "# ====================== Wiki-words-250-with-normalization =======================\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.63      0.83      0.72        77\n",
    "#            1       0.55      0.69      0.61       159\n",
    "#            2       0.53      0.61      0.57       303\n",
    "#            3       0.50      0.57      0.53       500\n",
    "#            4       0.87      0.75      0.81      1226\n",
    "\n",
    "#     accuracy                           0.69      2265\n",
    "#    macro avg       0.62      0.69      0.65      2265\n",
    "# weighted avg       0.71      0.69      0.70      2265\n",
    "\n",
    "# ====================== nnlm-en-dim128-with-normalization =======================\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.61      0.86      0.71        77\n",
    "#            1       0.56      0.69      0.62       159\n",
    "#            2       0.54      0.59      0.56       303\n",
    "#            3       0.49      0.56      0.52       500\n",
    "#            4       0.87      0.74      0.80      1226\n",
    "\n",
    "#     accuracy                           0.68      2265\n",
    "#    macro avg       0.61      0.69      0.64      2265\n",
    "# weighted avg       0.71      0.68      0.69      2265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================ Wiki-words-250 ================================\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.54      0.81      0.65        77\n",
    "#            1       0.52      0.68      0.59       159\n",
    "#            2       0.54      0.60      0.57       303\n",
    "#            3       0.48      0.54      0.51       500\n",
    "#            4       0.86      0.73      0.79      1226\n",
    "\n",
    "#     accuracy                           0.67      2265\n",
    "#    macro avg       0.59      0.67      0.62      2265\n",
    "# weighted avg       0.70      0.67      0.68      2265\n",
    "\n",
    "# ====================== Wiki-words-250-with-normalization =======================\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.63      0.83      0.72        77\n",
    "#            1       0.55      0.69      0.61       159\n",
    "#            2       0.53      0.61      0.57       303\n",
    "#            3       0.50      0.57      0.53       500\n",
    "#            4       0.87      0.75      0.81      1226\n",
    "\n",
    "#     accuracy                           0.69      2265\n",
    "#    macro avg       0.62      0.69      0.65      2265\n",
    "# weighted avg       0.71      0.69      0.70      2265\n",
    "\n",
    "# ====================== nnlm-en-dim128-with-normalization =======================\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.61      0.86      0.71        77\n",
    "#            1       0.56      0.69      0.62       159\n",
    "#            2       0.54      0.59      0.56       303\n",
    "#            3       0.49      0.56      0.52       500\n",
    "#            4       0.87      0.74      0.80      1226\n",
    "\n",
    "#     accuracy                           0.68      2265\n",
    "#    macro avg       0.61      0.69      0.64      2265\n",
    "# weighted avg       0.71      0.68      0.69      2265"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
